import logging
import validators
import json
import requests
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from modules import sqli, xss, open_redirect

# Setup logging
logging.basicConfig(filename='logs/webexploit.log', level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

SECURITY_HEADERS = [
    "X-Frame-Options", "Content-Security-Policy", "Strict-Transport-Security",
    "X-Content-Type-Options", "Referrer-Policy"
]

def validate_url(url):
    """Validate URL format."""
    if not validators.url(url):
        print("[❌] Invalid URL format. Please enter a valid URL (e.g., https://example.com)")
        return None
    return url

def scan_security_headers(url):
    """Checks security headers on the given URL."""
    print(f"\n[🔍] Checking security headers for {url}\n")
    try:
        response = requests.get(url, timeout=10)
        missing_headers = [h for h in SECURITY_HEADERS if h not in response.headers]

        if missing_headers:
            print(f"[⚠] Missing Headers: {', '.join(missing_headers)}")
        else:
            print("[✔] All security headers are present!")
    
    except requests.exceptions.RequestException as e:
        print(f"[ERROR] Failed to scan {url}: {e}")

def scan_vulnerabilities(url, scan_type, output_file):
    """Runs a specific vulnerability scan on the URL and saves output."""
    print(f"[🚀] Running {scan_type.upper()} scan on {url}")

    try:
        if scan_type == "sqli":
            return sqli.scan(url, output_file)
        elif scan_type == "xss":
            return xss.scan(url, output_file)
        elif scan_type == "redirect":
            return open_redirect.scan(url, output_file)
    except Exception as e:
        logging.error(f"Scan error: {e}")
        print(f"[ERROR] {e}")
    return {}

def crawl_website(url, depth):
    """Recursively crawls the website to discover links."""
    visited_links = set()

    def crawl(link, current_depth):
        if current_depth == 0 or link in visited_links:
            return
        visited_links.add(link)
        print(f"[🌐] Crawling: {link}")

        try:
            response = requests.get(link, timeout=5)
            soup = BeautifulSoup(response.text, "html.parser")
            for a_tag in soup.find_all("a", href=True):
                new_link = urljoin(url, a_tag["href"])
                crawl(new_link, current_depth - 1)
        except requests.exceptions.RequestException:
            pass

    crawl(url, depth)
    return visited_links

def save_results(results, output_file, output_format):
    """Save scan results to a file."""
    with open(output_file, "w") as f:
        if output_format == "json":
            json.dump(results, f, indent=4)
        else:
            for key, value in results.items():
                f.write(f"{key}: {value}\n")
    print(f"[📁] Scan results saved in {output_file}")

def main():
    print("""
    ██████╗ ███████╗████████╗████████╗███████╗ █████╗ ███╗   ███╗
    ██╔══██╗██╔════╝╚══██╔══╝╚══██╔══╝██╔════╝██╔══██╗████╗ ████║
    ██║  ██║█████╗     ██║      ██║   █████╗  ███████║██╔████╔██║
    ██║  ██║██╔══╝     ██║      ██║   ██╔══╝  ██╔══██║██║╚██╔╝██║
    ██████╔╝███████╗   ██║      ██║   ███████╗██║  ██║██║ ╚═╝ ██║
    ╚═════╝ ╚══════╝   ╚═╝      ╚═╝   ╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝
    🔥 REDTEAM SCANNER - A Powerful Web Exploitation & Security Tool 🔥
    """)

    url = None
    while url is None:
        url = validate_url(input("Enter Target URL (e.g., https://example.com): ").strip())

    while True:
        print("\n[🔹] Select an option:")
        print("1️⃣  SQL Injection Scan")
        print("2️⃣  XSS Scan")
        print("3️⃣  Open Redirect Scan")
        print("4️⃣  Security Headers Check")
        print("5️⃣  Web Crawler")
        print("0️⃣  Exit")

        choice = input("Enter your choice (1-5 or 0 to exit): ").strip()

        if choice == "0":
            print("[👋] Exiting RedTeam Scanner. Stay secure!")
            break
        elif choice in ["1", "2", "3"]:
            scan_type = {"1": "sqli", "2": "xss", "3": "redirect"}[choice]
            output_file = f"reports/{scan_type}_report.txt"
            results = scan_vulnerabilities(url, scan_type, output_file)
            save_results(results, output_file, "txt")
        elif choice == "4":
            scan_security_headers(url)
        elif choice == "5":
            depth = int(input("Enter crawling depth (default 2): ").strip() or 2)
            results = {"crawled_links": list(crawl_website(url, depth))}
            output_file = "reports/crawled_links.json"
            save_results(results, output_file, "json")
        else:
            print("[❌] Invalid choice! Please select a valid option.")

if __name__ == "__main__":
    main()
