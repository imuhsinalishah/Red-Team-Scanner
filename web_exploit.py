import logging
import validators
import json
import requests
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from modules import sqli, xss, open_redirect

# Setup logging
logging.basicConfig(filename='logs/webexploit.log', level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

SECURITY_HEADERS = [
    "X-Frame-Options", "Content-Security-Policy", "Strict-Transport-Security",
    "X-Content-Type-Options", "Referrer-Policy"
]

def validate_url(url):
    """Validate URL format."""
    if not validators.url(url):
        print("[âŒ] Invalid URL format. Please enter a valid URL (e.g., https://example.com)")
        return None
    return url

def scan_security_headers(url):
    """Checks security headers on the given URL."""
    print(f"\n[ğŸ”] Checking security headers for {url}\n")
    try:
        response = requests.get(url, timeout=10)
        missing_headers = [h for h in SECURITY_HEADERS if h not in response.headers]

        if missing_headers:
            print(f"[âš ] Missing Headers: {', '.join(missing_headers)}")
        else:
            print("[âœ”] All security headers are present!")
    
    except requests.exceptions.RequestException as e:
        print(f"[ERROR] Failed to scan {url}: {e}")

def scan_vulnerabilities(url, scan_type, output_file):
    """Runs a specific vulnerability scan on the URL and saves output."""
    print(f"[ğŸš€] Running {scan_type.upper()} scan on {url}")

    try:
        if scan_type == "sqli":
            return sqli.scan(url, output_file)
        elif scan_type == "xss":
            return xss.scan(url, output_file)
        elif scan_type == "redirect":
            return open_redirect.scan(url, output_file)
    except Exception as e:
        logging.error(f"Scan error: {e}")
        print(f"[ERROR] {e}")
    return {}

def crawl_website(url, depth):
    """Recursively crawls the website to discover links."""
    visited_links = set()

    def crawl(link, current_depth):
        if current_depth == 0 or link in visited_links:
            return
        visited_links.add(link)
        print(f"[ğŸŒ] Crawling: {link}")

        try:
            response = requests.get(link, timeout=5)
            soup = BeautifulSoup(response.text, "html.parser")
            for a_tag in soup.find_all("a", href=True):
                new_link = urljoin(url, a_tag["href"])
                crawl(new_link, current_depth - 1)
        except requests.exceptions.RequestException:
            pass

    crawl(url, depth)
    return visited_links

def save_results(results, output_file, output_format):
    """Save scan results to a file."""
    with open(output_file, "w") as f:
        if output_format == "json":
            json.dump(results, f, indent=4)
        else:
            for key, value in results.items():
                f.write(f"{key}: {value}\n")
    print(f"[ğŸ“] Scan results saved in {output_file}")

def main():
    print("""
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•     â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘
    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•   â•šâ•â•      â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•
    ğŸ”¥ REDTEAM SCANNER - A Powerful Web Exploitation & Security Tool ğŸ”¥
    """)

    url = None
    while url is None:
        url = validate_url(input("Enter Target URL (e.g., https://example.com): ").strip())

    while True:
        print("\n[ğŸ”¹] Select an option:")
        print("1ï¸âƒ£  SQL Injection Scan")
        print("2ï¸âƒ£  XSS Scan")
        print("3ï¸âƒ£  Open Redirect Scan")
        print("4ï¸âƒ£  Security Headers Check")
        print("5ï¸âƒ£  Web Crawler")
        print("0ï¸âƒ£  Exit")

        choice = input("Enter your choice (1-5 or 0 to exit): ").strip()

        if choice == "0":
            print("[ğŸ‘‹] Exiting RedTeam Scanner. Stay secure!")
            break
        elif choice in ["1", "2", "3"]:
            scan_type = {"1": "sqli", "2": "xss", "3": "redirect"}[choice]
            output_file = f"reports/{scan_type}_report.txt"
            results = scan_vulnerabilities(url, scan_type, output_file)
            save_results(results, output_file, "txt")
        elif choice == "4":
            scan_security_headers(url)
        elif choice == "5":
            depth = int(input("Enter crawling depth (default 2): ").strip() or 2)
            results = {"crawled_links": list(crawl_website(url, depth))}
            output_file = "reports/crawled_links.json"
            save_results(results, output_file, "json")
        else:
            print("[âŒ] Invalid choice! Please select a valid option.")

if __name__ == "__main__":
    main()
